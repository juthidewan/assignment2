---
title: 'Assignment #2'
output: 
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    code_download: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

```{r libraries}
# SEE modeldata package for new datasets
library(tidyverse)         # for graphing and data cleaning
library(tidymodels)        # for modeling
library(stacks)            # for stacking models
library(naniar)            # for examining missing values (NAs)
library(lubridate)         # for date manipulation
library(moderndive)        # for King County housing data
library(vip)               # for variable importance plots
library(DALEX)             # for model interpretation  
library(DALEXtra)          # for extension of DALEX
library(patchwork)         # for combining plots nicely
theme_set(theme_minimal()) # Lisa's favorite theme
```

```{r data}
data("lending_club")
# Data dictionary (as close as I could find): https://www.kaggle.com/wordsforthewise/lending-club/discussion/170691
```


When you finish the assignment, remove the `#` from the options chunk at the top, so that messages and warnings aren't printed. If you are getting errors in your code, add `error = TRUE` so that the file knits. I would recommend not removing the `#` until you are completely finished.

## Put it on GitHub!        

From now on, GitHub should be part of your routine when doing assignments. I recommend making it part of your process anytime you are working in R, but I'll make you show it's part of your process for assignments.

**Task**: When you are finished with the assignment, post a link below to the GitHub repo for the assignment. If you want to post it to your personal website, that's ok (not required). Make sure the link goes to a spot in the repo where I can easily find this assignment. For example, if you have a website with a blog and post the assignment as a blog post, link to the post's folder in the repo. As an example, I've linked to my GitHub stacking material [here](https://github.com/llendway/ads_website/tree/master/_posts/2021-03-22-stacking).


### GitHub repo link

```{r}
#https://github.com/juthidewan/assignment2/blob/main/02_assignment.Rmd
library(readxl)
data_dictionary <- read_excel("LCDataDictionary.xlsx")
```



## Modeling

Before jumping into these problems, you should read through (and follow along with!) the [model stacking](https://advanced-ds-in-r.netlify.app/posts/2021-03-22-stacking/) and [global model interpretation](https://advanced-ds-in-r.netlify.app/posts/2021-03-24-imlglobal/) tutorials on the Course Materials tab of the course website.

We'll be using the `lending_club` dataset from the `modeldata` library, which is part of `tidymodels`. The data dictionary they reference doesn't seem to exist anymore, but it seems the one on this [kaggle discussion](https://www.kaggle.com/wordsforthewise/lending-club/discussion/170691) is pretty close. It might also help to read a bit about [Lending Club](https://en.wikipedia.org/wiki/LendingClub) before starting in on the exercises.

The outcome we are interested in predicting is `Class`. And according to the dataset's help page, its values are "either 'good' (meaning that the loan was fully paid back or currently on-time) or 'bad' (charged off, defaulted, of 21-120 days late)".


1. Explore the data, concentrating on examining distributions of variables and examining missing values. 

### Exercise 1

<br> 

```{r}
lending_club <- 
  lending_club %>%
  select(-delinq_amnt)

lending_club %>% 
  add_n_miss() %>% 
  count(n_miss_all)
```


```{r}
lending_club$emp_length <- gsub('emp_1', '1', lending_club$emp_length)
lending_club$emp_length <- gsub('emp_5', '5', lending_club$emp_length)
lending_club$emp_length <- gsub('emp_6', '6', lending_club$emp_length)
lending_club$emp_length <- gsub('emp_2', '2', lending_club$emp_length)
lending_club$emp_length <- gsub('emp_3', '3', lending_club$emp_length)
lending_club$emp_length <- gsub('emp_4', '4', lending_club$emp_length)
lending_club$emp_length <- gsub('emp_7', '7', lending_club$emp_length)
lending_club$emp_length <- gsub('emp_8', '8', lending_club$emp_length)
lending_club$emp_length <- gsub('emp_9', '9', lending_club$emp_length)
lending_club$emp_length <- gsub('emp_10', '10', lending_club$emp_length)
lending_club$emp_length <- gsub('emp_ge_10', '10', lending_club$emp_length)
lending_club$emp_length <- gsub('emp_lt_1', '0', lending_club$emp_length)
lending_club$emp_length <- gsub('emp_unk', '0', lending_club$emp_length)

lending_club$emp_length <- as.numeric(lending_club$emp_length)
```

<br> 

##### There are no missing values in the dataset. But the data keeps track of delinquencies, the number of different installment accounts opened over the span of a year, number of personal accounts, the amounts loaned out, interest rate and state of residence. I took out the sub_grade variable because I don't really think we will be needing it as it is the same as the Class variable just more niche. I also cleaned up the emp_length variable into numbers and got rid of any words. I also changed emp_ge_10 to 10, emp_lt_1 to 0, and emp_unk to 0. 

<br> 

### Exercise 2

```{r}
set.seed(494)
create_more_bad <- lending_club %>% 
  filter(Class == "bad") %>% 
  sample_n(size = 3000, replace = TRUE)

lending_club_mod <- lending_club %>% 
  bind_rows(create_more_bad) 

lending_club_mod$emp_length <- as.numeric(lending_club_mod$emp_length)
```


<br> 

### Exercise 3

```{r}
set.seed(494) # for reproducibility

# Randomly assigns 75% of the data to training.
LC_split <- initial_split(lending_club_mod, 
                             prop = .75)

LC_training <- training(LC_split)
LC_testing <- testing(LC_split)
```


<br> 

### Exercise 4 

```{r}
set.seed(494) #for reproducible 5-fold

LC_recipe <- recipe(Class ~ .,
                       data = LC_training) %>%
  step_mutate_at(term, fn = ~ as.numeric(fct_lump_n(term, n=2)))%>% 
  step_mutate_at(term, fn = ~as.numeric(. >1)) %>%
  step_mutate_at(delinq_2yrs, fn = ~as.numeric(. >0)) %>%
  step_mutate_at(inq_last_6mths, fn = ~as.numeric(. >0)) %>%
  step_mutate_at(acc_now_delinq, fn = ~as.numeric(. >0)) %>%
  step_mutate_at(open_il_6m, fn = ~as.numeric(. >0)) %>%
  step_mutate_at(open_il_12m, fn = ~as.numeric(. >0)) %>%
  step_mutate_at(open_il_24m, fn = ~as.numeric(. >0)) %>%
  step_mutate_at(inq_fi, fn = ~as.numeric(. >0)) %>%
  step_mutate_at(inq_last_12m, fn = ~as.numeric(. >0)) %>%
  step_mutate_at(num_il_tl, fn = ~as.numeric(. >0)) %>%
  step_mutate_at(emp_length, fn = ~as.numeric(. >0)) %>%
  step_mutate(
    sub_grade = fct_collapse(sub_grade,
                             A = c("A1", "A2", "A3", "A4", "A5"),
                             B = c("B1", "B2", "B3", "B4", "B5"),
                             C = c("C1", "C2", "C3", "C4", "C5"),
                             D = c("D1", "D2", "D3", "D4", "D5"),
                             E = c("E1", "E2", "E3", "E4", "E5"),
                             f = c("F1", "F2", "F3", "F4", "F5"),
                             G = c("G1", "G2", "G3", "G4", "G5")),
    addr_state = fct_collapse(addr_state,
                              West = c("AK","AZ", "CA", "CO", "HI", "ID", "MT", "NM", "NV","OR","UT", "WA", "WY"  ),
                              South = c("AL", "AR", "DC", "DE", "FL","GA", "KY", "LA", "MD", "MS", "NC", "OK", "SC", "TN", "TX", "VA", "WV"),
                              Northeast = c("CT","MA", "ME","NH", "NJ", "NY", "PA", "RI","VT"),
                              Midwest = c("IA", "IL", "IN", "KS", "MI", "MN", "MO", "ND", "NE", "OH", "SD", "WI"))) %>%
   step_normalize(all_predictors(), 
                 -all_nominal()) %>%
   step_dummy(all_nominal(),
             -all_outcomes()) 
```

<br> 

### Exercise 5 

```{r}
#define lasso model
LC_lasso_mod <- 
  logistic_reg(mixture = 1) %>% 
  set_engine("glmnet") %>% 
  set_args(penalty = tune()) %>% 
  set_mode("classification")

LC_lasso_wf <- 
  workflow() %>% 
  add_recipe(LC_recipe) %>% 
  add_model(LC_lasso_mod)

LC_lasso_fit <- 
  LC_lasso_wf %>%
  fit(LC_training)

LC_lasso_fit
```


<br> 

### Exercise 6 

```{r}
set.seed(494)
LC_cv <- vfold_cv(LC_training, v = 5)

penalty_grid <- grid_regular(penalty(),
                             levels = 10)

ctrl_grid <- control_stack_grid()

LC_lasso_tune <- 
  LC_lasso_wf %>% 
  tune_grid(
    resamples = LC_cv,
    grid = penalty_grid,
    control = ctrl_grid
    )

LC_lasso_tune %>%
  collect_metrics()

```

<br>

##### The mean accuracy for the best parameter is 0.7495590

```{r}
best_param <- LC_lasso_tune %>% 
  select_best(metric = "accuracy")
best_param

LC_lasso_final_wf <- LC_lasso_wf %>% 
  finalize_workflow(best_param)
LC_lasso_final_wf

LC_lasso_test <- LC_lasso_final_wf %>% 
  last_fit(LC_split)

LC_lasso_test %>%
  collect_metrics(LC_cv)

lasso_fit <- 
  LC_lasso_final_wf %>%
  fit(data = LC_training)
```

<br> 

##### The area under the ROC curve is 0.7710287 

<br> 

### Exercise 7

```{r}
set.seed(494) #for reproducible 5-fold

forest_recipe <- 
  recipe(formula = Class ~ ., 
         data = LC_training)
```

<br> 

### Exercise 8

```{r}
forest_mod <- 
  rand_forest(mtry=tune(), 
              min_n=tune(), 
              trees = 100) %>% 
  set_mode("classification") %>% 
  set_engine("ranger")

forest_wf <- 
  workflow() %>% 
  add_recipe(forest_recipe) %>% 
  add_model(forest_mod) 
```

<br> 

### Exercise 9

```{r}
rf_penalty_grid <- 
  grid_regular(finalize(mtry(),
                        LC_training %>%
                          select(-Class)),
               min_n(),
               levels = 3)

forest_tune <- 
  forest_wf %>% 
  tune_grid(
    resamples = LC_cv,
    grid = rf_penalty_grid,
    control = control_stack_grid()
    )

forest_tune
```

### Exercise 10


```{r}
forest_tune %>%
  collect_metrics()

best_param_rf <- forest_tune %>% 
  select_best(metric = "accuracy")
best_param_rf

forest_final_wf <- forest_wf %>% 
  finalize_workflow(best_param_rf)
forest_final_wf

forest_fit <- 
  forest_final_wf %>%
  fit(data = LC_training)
```


<br> 

##### The accuracy is 0.9884892. The area under the ROC curve is 0.9997541. 

<br> 

### Exercise 11


```{r}
explainer_lasso <- 
  explain_tidymodels(
    model = lasso_fit,
    data = LC_training %>% select(-Class), 
    y = LC_training %>%
      mutate(Class_num = as.integer(Class =="good")) %>%
      pull(Class_num),
    label = "lasso"
  )

explainer_rf <- 
  explain_tidymodels(
    model = forest_fit,
    data = LC_training %>% select(-Class), 
    y = LC_training %>%
      mutate(Class_num = as.integer(Class =="good")) %>%
      pull(Class_num),
    label = "rf"
  )

rf_mod_perf <-  model_performance(explainer_rf)
lasso_mod_perf <-  model_performance(explainer_lasso)

hist_plot <- 
  plot(rf_mod_perf, 
       lasso_mod_perf,
       geom = "histogram")

box_plot <-
  plot(rf_mod_perf, 
       lasso_mod_perf,
       geom = "boxplot")

hist_plot + box_plot
```

<br> 

##### The lasso has a smaller spread of residuals around 0 whereas the the random forest has a larger spread. The peak count for lasso is also higher than for random forest. The boxplot reflects the same information as the histograms. The rf has a smaller root mean square of residuals whereas the lasso has a much bigger root mean square of residuals. 


<br> 

### Exercise 12


```{r}
set.seed(10) #since we are sampling & permuting, we set a seed so we can replicate the results
rf_var_imp <- 
  model_parts(
    explainer_rf
    )

lasso_var_imp <- 
  model_parts(
    explainer_lasso
    )

plot(rf_var_imp, show_boxplots = TRUE)
plot(lasso_var_imp, show_boxplots = TRUE)

lasso_fit%>% 
  pull_workflow_fit() %>% 
  vip()

```
<br> 

##### The two different models give different varibales. For the rf, the most important variable is open_il_12m with sub_grade coming in second, but with a significant different.  For the lasso model, the most important variable is int_rate with open_il_24m following behind but with a significant difference as well. 


<br>

### Exercise 13

```{r}
cp_profile <- function(exp, obs, var){
  
rf_cpp <- predict_profile(explainer = exp, 
                          new_observation = obs, 
                          variables = var) %>%
  ggplot(aes_string(x = var, 
                    y= "`_yhat_`")) + 
  geom_line()
rf_cpp
}
```

```{r}
cp_profile(explainer_rf, LC_training %>% slice(3), "int_rate")
```


14. Use `DALEX` functions to create partial dependence plots (with the CP profiles in gray) for the 3-4 most important variables. If the important variables are categorical, you can instead make a CP profile for 3 observations in the dataset and discuss how you could go about constructing a partial dependence plot for a categorical variable (you don't have to code it, but you can if you want an extra challenge). If it ever gives you an error that says, "Error: Can't convert from `VARIABLE` <double> to `VARIABLE` <integer> due to loss of precision", then remove that variable from the list. I seem to have figured out why it's doing that, but I don't know how to fix it yet.

```{r}
set.seed(494) # since we take a sample of 100 obs
# This takes a while to run. 
# If we only want to examine a few variables, add variables argument to model_profile.


rf_pdp_1 <- model_profile(explainer = explainer_rf, variables = c("open_il_24m"))
lasso_pdp_1 <- model_profile(explainer = explainer_lasso, variables = c("open_il_24m"))

rf_pdp_2 <- model_profile(explainer = explainer_rf, variables = c("sub_grade"))
lasso_pdp_2 <- model_profile(explainer = explainer_lasso, variables = c("sub_grade"))


rf_pdp_3 <- model_profile(explainer = explainer_rf, variables = c("acc_now_delinq"))
lasso_pdp_3 <- model_profile(explainer = explainer_lasso, variables = c("acc_now_delinq"))

plot(rf_pdp_1,
     variables ="open_il_24m",
     geom = "profiles")
plot(lasso_pdp_1,
     variables ="open_il_24m",
     geom = "profiles")


plot(rf_pdp_2,
     variables ="sub_grade",
     geom = "profiles")
plot(lasso_pdp_2,
     variables ="sub_grade",
     geom = "profiles")




plot(rf_pdp_3,
     variables ="acc_now_delinq",
     geom = "profiles")

```



### Exercise 15 

```{r}
#knn 

# create a model definition
knn_mod <-
  nearest_neighbor(
    neighbors = tune("k")
  ) %>%
  set_engine("kknn") %>% 
  set_mode("classification")

# create the workflow
knn_wf <- 
  workflow() %>% 
  add_model(knn_mod) %>%
  add_recipe(LC_recipe)

# tune it using 4 tuning parameters
knn_tune <- 
  knn_wf %>% 
  tune_grid(
    LC_cv,
    grid = 4,
    control = ctrl_grid
  )
```


### Exercise 16

```{r}
#set.seed(1211) # for reproducibility
#LC_cv <- vfold_cv(LC_training, v = 5)

#ctrl_res <- control_stack_resamples()

#forest_cv <- forest_fit %>% 
#  fit_resamples(LC_cv, 
 #                control = ctrl_res)

#LC_stack <- 
  #stacks() %>% 
  #add_candidates(forest_cv) %>% 
  #add_candidates(LC_lasso_tune) %>% 
  #add_candidates(knn_tune)

#as_tibble(LC_stack)

#LC_blend <- 
#  LC_stack %>% 
#  blend_predictions()

#LC_blend

#autoplot(LC_blend)

#autoplot(LC_blend, type = "weights")
```


<br> 

##### rand_forest is contributing most. 

### Exercise 17

17. Fit the final stacked model using `fit_members()`. Apply the model to the test data and report the accuracy and area under the curve. Create a graph of the ROC and construct a confusion matrix. Comment on what you see. Save this final model using the `saveRDS()` function - see the [Use the model](https://advanced-ds-in-r.netlify.app/posts/2021-03-16-ml-review/#use-the-model) section of the `tidymodels` intro. We are going to use the model in the next part. You'll want to save it in the folder where you create your shiny app.

```{r}
#LC_final_stack <- LC_blend %>% 
#  fit_members()


#LC_final_stack %>% 
#  predict(new_data = LC_testing, type="prob") %>% 
#  bind_cols(LC_testing) %>% 
#  ggplot(aes(x = Class, 
#             y = .pred_good)) +
#  geom_point(alpha = .5, 
#              size = .5) +
#   labs(x = "Actual Class", 
#        y = "Predicted Class (Good)")
```



## Coded Bias

Watch the [Code Bias](https://www.pbs.org/independentlens/films/coded-bias/) film and write a short reflection. If you want some prompts, reflect on: What part of the film impacted you the most? Was there a part that surprised you and why? What emotions did you experience while watching?


##### These are some parts that left an impression on me : Over 90% of the facial recognition matching was wrong. The police using facial recognition sounds dangerous. Guy got fined for not wanting to show his face. Scenes from the HK protest were really cool. The lasers, the masks so that protestors would not be found out. Black and brown communities are targeted bc there is a lower expectation of privacy. The poor communities are the ones where these technologies are tested. apple job applications - it's replicating the world as it is - not ethical. If there's already less women in tech, the algorithm replicates that and is biased weeding out women. The evil robot Tay tweeting out and saying hateful things. For the most part of the movie, I felt like there was this whole another world that I knew nothing about. It felt like all of these algorithms were invisible and not part of my world at all. But the ending made me feel hopeful. 


REMEMBER TO ADD YOUR GITHUB LINK AT THE TOP OF THE PAGE AND UNCOMMENT THE `knitr` OPTIONS.


